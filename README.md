# Neural Network & Deep Learning projects

# assignment 1
1. Compiled basic classifiers: logistic regression and softmax classifier, including implementation of all layer functions.
2. Designed and trained a two layers neural network and a multi-layer-perception (MLP) model using numpy. All the layer functions and back-propagation are implement manually, without calling tensorflow.
3. Using tensorflow library implemented model in 2

# assignment 2
1. Implemented 4 optimization method: SGD with Momentum, Adagrad, Adam and Nadam and compared performance.
2. Regularization layers including Dropout and Batch Normalization.
3. Manually implemented image data augmentation functions and trained a CNN model.
4. Bottle Kaggle Competition. An in-class Kaggle competition using CNN to detect how much liquid does a bottle have.

# assignment 3
1. RNN application -- XNOR
Built and trained an XNOR network that can learn the XNOR function. Training data is generated by xnor function. 

2. RNN application --Tweet Sentiment Analysis
Encoded the data using one hot encoding and train an LSTM network to classify the sentiment. Replaced the one hot encoding with an embedding layer and train another LSTM model. 

# LSTM revenue data prediction
An application using RNN LSTM to predict real revenue data. Data is provided by a music recording company collected from past 2 years. It turns out the model can fit past revenue well. But it predicts less accurate in the pattern with many impulses.

# Residual Attention Network
"Residual Attention Network", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Authors propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. 
